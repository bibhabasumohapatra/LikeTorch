{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Patchify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, n_inputs, n_neurons):\n",
    "        self.weights = np.random.uniform(low=0., high=1.,size=(n_inputs, n_neurons))\n",
    "        self.biases = np.zeros((1, n_neurons))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.output = np.dot(inputs, self.weights) + self.biases\n",
    "        return self.output\n",
    "    \n",
    "class Dropout:\n",
    "    def __init__(self,p=0.5):\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, x):\n",
    "        drop = np.random.uniform(low=0., high=1., size=x.shape) > self.p\n",
    "        x = np.multiply(drop,x)\n",
    "        return x\n",
    "    \n",
    "class sigmoid:\n",
    "    def __init__(self,):\n",
    "        pass\n",
    "    def forward(self,x):\n",
    "        return 1/(1+np.exp(x))\n",
    "    \n",
    "class ReLu:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    def forward(self,x):\n",
    "        return np.maximum(0,x)\n",
    "    \n",
    "class Softmax:\n",
    "    def __init__(self, ):\n",
    "        pass\n",
    "    def forward(self,x):\n",
    "        x = np.exp(x)/np.sum(np.exp(x),axis=0)  ## Row-Wise\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Patch_Generator:\n",
    "    def __init__(self, kernel=(3,16,16),stride=(1,1)):\n",
    "        self.kernel = np.ones(kernel)\n",
    "        self.stride = stride\n",
    "\n",
    "    def pad(self,image, pad_y, pad_x):\n",
    "        image = np.pad(\n",
    "            image,\n",
    "            ((0,0),(pad_y, pad_y), (pad_x, pad_x)),\n",
    "            mode=\"constant\",\n",
    "            constant_values=0,\n",
    "        )  ## PADDING confirm\n",
    "        return image\n",
    "\n",
    "    def forward(self, image): ## H, W\n",
    "        \n",
    "        Yc, Xc = image.shape[1]//2, image.shape[2]//2\n",
    "        k_len = self.kernel.shape[1]\n",
    "\n",
    "        padding = k_len // 2\n",
    "        patches = []\n",
    "        out = []\n",
    "        image = self.pad(image,padding,padding)\n",
    "        for i in range(0,image.shape[1]-k_len,16):\n",
    "            for j in range(0,image.shape[2]-k_len,16):\n",
    "                iteration = np.multiply(image[:,i:i+k_len,j:j+k_len],self.kernel)\n",
    "                patches.append(iteration)\n",
    "                out.append(iteration.flatten())\n",
    "        return {\n",
    "                 \"patch\" : np.array(patches),\n",
    "                 \"flatten_patches\":np.array(out)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Projection_Layer:\n",
    "    def __init__(self,):\n",
    "        self.layer_1 = Linear(768,512)\n",
    "        self.layer_2 = Linear(512,512)\n",
    "        self.drop = Dropout(0.5)\n",
    "        self.relu = ReLu()\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = self.layer_1.forward(x)\n",
    "        x = self.layer_2.forward(x)\n",
    "        x = self.drop.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class Multiply_k_q:\n",
    "    def __init__(self,):\n",
    "        pass\n",
    "    def forward(self,k,q):\n",
    "        return np.matmul(q,k.T)\n",
    "    \n",
    "class Multiply_k_q_v:\n",
    "    def __init__(self,):\n",
    "        pass\n",
    "    def forward(self,k_q,v):\n",
    "        return np.matmul(k_q,v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(196, 3, 16, 16)\n",
      "(196, 768)\n",
      "(196, 512)\n"
     ]
    }
   ],
   "source": [
    "dummy_image = np.random.rand(3,224,224)\n",
    "im = Patch_Generator().forward(dummy_image)\n",
    "print(im[\"patch\"].shape)\n",
    "print(im[\"flatten_patches\"].shape)\n",
    "im = Projection_Layer().forward(im['flatten_patches'])\n",
    "print(im.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Simple_Transformer:\n",
    "    def __init__(self,):\n",
    "        self.patching = Patch_Generator()\n",
    "        self.projection = Projection_Layer()\n",
    "        self.init_rand = np.random.uniform(-0.1,0.1,size=(196,512))\n",
    "        self.W_q_1 = Linear(512,64)\n",
    "        self.W_k_1 = Linear(512,64)\n",
    "        self.W_v_1 = Linear(512,64)\n",
    "\n",
    "        self.W_q_2 = Linear(512,64)\n",
    "        self.W_k_2 = Linear(512,64)\n",
    "        self.W_v_2 = Linear(512,64)\n",
    "        \n",
    "        self.mul_q_k_1 = Multiply_k_q()\n",
    "        self.mul_q_k_2 = Multiply_k_q()\n",
    "\n",
    "        self.mul_q_k_v_1 = Multiply_k_q_v()\n",
    "        self.mul_q_k_v_2 = Multiply_k_q_v()\n",
    "\n",
    "        self.softmax = Softmax()\n",
    "\n",
    "        self.fc = Linear(128,512)\n",
    "        self.dropout = Dropout()\n",
    "        self.relu = ReLu()\n",
    "        self.final_layer = Linear(512,128)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.patching.forward(dummy_image)['flatten_patches']  ## 196,768\n",
    "        x = self.projection.forward(x)  ## 196,512\n",
    "        x = x + self.init_rand          ## 196,512\n",
    "\n",
    "        k_1 = self.W_k_1.forward(x)   ## 196,64\n",
    "        k_2 = self.W_k_2.forward(x)   ## 196,64\n",
    "\n",
    "        q_1 = self.W_q_1.forward(x)   ## 196,64\n",
    "        q_2 = self.W_q_2.forward(x)   ## 196,64\n",
    "\n",
    "        v_1 = self.W_v_1.forward(x)   ## 196,64\n",
    "        v_2 = self.W_v_2.forward(x)   ## 196,64\n",
    "\n",
    "        k_1_q_1 = self.mul_q_k_1.forward(k_1,q_1)  ## 196,196\n",
    "        k_2_q_2 = self.mul_q_k_2.forward(k_2,q_2)  ## 196,196\n",
    "\n",
    "        out_1 = self.softmax.forward(k_1_q_1)  ## 196,196\n",
    "        out_2 = self.softmax.forward(k_2_q_2)  ## 196,196\n",
    "\n",
    "        x_1 = self.mul_q_k_v_1.forward(out_1,v_1) ## 196x64\n",
    "        x_2 = self.mul_q_k_v_2.forward(out_2,v_2) ## 196x64\n",
    "\n",
    "        x = np.concatenate([x_1,x_2],axis=1)  ## 196x128\n",
    "        \n",
    "        x = self.fc.forward(x)\n",
    "        x = self.dropout.forward(x)\n",
    "        x = self.relu.forward(x)\n",
    "        x = self.final_layer.forward(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(196, 128)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_image = np.random.rand(3,224,224).astype(np.int8)\n",
    "Simple_Transformer().forward(dummy_image).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MRI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
